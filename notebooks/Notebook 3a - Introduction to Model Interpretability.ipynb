{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e70690",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/leolorenzoii/ml2_interpretability/blob/main/notebooks/01_Model_Interpretability_and_Shapley_Values.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bc83a",
   "metadata": {},
   "source": [
    "# Introduction to Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44863d11",
   "metadata": {},
   "source": [
    "Throughout the **Machine Learning 1** course, we learned all about different machine learning algorithms- how they work, how we can hypertune their hyperparameters, and how we can select the optimal model through a cross validation strategy. In the end, we can now train a machine learning model and use it to make predictions on unseen data (see Figure <a href='#fig:ml-nutshell'>1</a>).\n",
    "\n",
    "<a name='fig:ml-nutshell'></a>\n",
    "<div>\n",
    "<img src=\"images/ml-nutshell.png\" align=\"left\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 1. Training and testing a machine learning model in a nutshell.</b><br>\n",
    "        We learned how to train a machine learning model and use it to predict unseen data.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "However, our methodology is currently geared more towards only optimizing the predictive power of our models and less on how we can use our models for inference. In times where stakeholders require us to explain the predictions of our machine learning model, our current methodology will be insufficient. In particular, we lack the capacity to answer questions such as:\n",
    "\n",
    "- How does one (or more) feature impact the predictions of the model?\n",
    "- What is the role that each feature value play in each individual predictions?\n",
    "- How can we explain the models predictions in a more useful manner for our stakeholders?\n",
    "\n",
    "We will deal with methods that improves the explainability of our models in the next series of notebooks. For our first notebook on interpretability, a brief introduction on model interpretability and its importance will be emphasized. We also show in the concluding section of this notebook how we can incorporate these explainability methods on our machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff86f",
   "metadata": {},
   "source": [
    "## What is model interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad8798",
   "metadata": {},
   "source": [
    "In Christoph Molnar's [Interpretable Machine Learning](#ref:molnar) book [[2]](#ref:molnar), he collated two definitions of **interpretability**. A non-mathematical one:\n",
    "\n",
    "> *Interpretability is the degree to which a* ***human*** *can* ***understand*** *the cause of a* ***decision***. [[3]](#ref:miller)\n",
    "\n",
    "And a mathematical one:\n",
    "\n",
    "> *Interpretability is the degree to which a* ***human*** *can consistently* ***predict*** *the* ***model’s result***. [[4]](#ref:kim)\n",
    "\n",
    "In both definitions we see three important elements: the **human**, the **understanding**, and the **decision**. Thus, in the same way, when we construct our definition for *interpretability* in the context of machine learning:\n",
    "\n",
    "> ***Model interpretability*** *refers to the degree in which the behaviors and tendencies of statistical and machine learning models are understandable to humans.*\n",
    "\n",
    "Notice here that there is a *flexibility* in the definition of model interpretability. Indeed, defining how explicable a machine learning model is depends on the needs and requirements of a project and the different stakeholders.\n",
    "\n",
    "Nevertheless, as ethical machine learning practitioners and data science leaders, model interpretability MUST be integrated as early as possible in the development process and should NOT just be taken as an afterthought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275578aa",
   "metadata": {},
   "source": [
    "## Why interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0b6d2",
   "metadata": {},
   "source": [
    "Now, one might ask, why bother with interpretability? Wouldn't a high model performance would ultimately yield to higher business value? This is quite a arguable topic! In fact, in 2017, the [Neural Information Processing Systems](https://nips.cc/) conference in 2017 had its first ever ***The Great AI Debate*** with the topic ***Is interpretability necessary for machine learning?*** [[6]](#ref:great-ai-debate) *(you are highly encourage to watch this engaging and insightful discussion* 🙂).\n",
    "\n",
    "In the video, it was shown that model interpretability is crucial especially for some applications where quirky patterns from the data may be learned by the model. Indeed, having an interpretability pipeline in your project gives you, the Data Scientist, the ability to debug your model and identify issues early on. Thus, giving you a chance to improve your model. Furthermore, it has added benefits for other stakeholders that is interested and affected by your machine learning model (see Figure [4](#fig:stakeholders)).\n",
    "\n",
    "<a name='fig:stakeholders'></a>\n",
    "<div>\n",
    "<img src=\"images/interpretability-stakeholders.png\" align=\"left\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 4. Benefits of model interpretability to various stakeholders of a machine learning project.</b><br>\n",
    "           Model interpretability benefits the data scientist, business decision makers, approving authorities, and business customers.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "For **data scientists**, being able to explain the model to other stakeholders is also one benefit of having model explicability. The better you can explain the model to other people in the business, the greater its chance of being adopted and the trust given to the model by other stakeholders. With model interpretability, **business executives** now has the option to provide transparency to its end-users. Furthermore, it helps them justify the business case for the investment and identify other potential extensions and business use-case for the project. **Approving authorities** also benefit from model interpretability, by having a clear understanding of the risk the business is going to take in adopting the model, understanding the impact of the model decisions to humans, and anticipate any legal or regulatory issues that the model may face. Finally, **customer** experience and decision making can also be improved if they understand why a model gives a certain prediction.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "\n",
    "Here, we outlined the benefits of having an interpretability pipeline in our machine learning projects. Can you think of cases where having model interpretability is NOT preferred? Give a particular instance where having model interpretability can do more harm than good.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33556941",
   "metadata": {},
   "source": [
    "## What makes a good explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dfe86",
   "metadata": {},
   "source": [
    "Before we begin explaining predictions of machine learning models, it helps to understand what makes an explanation good an acceptable for humans. We are making explanations, after all, for a humans to be digested. This help us better frame the model explanations we get from model interpretability methods.\n",
    "\n",
    "Here are some of the few important characteristics of a good explanation *(see Chapter 3.6 of Molnar's Interpretable Machine Learning for a complete list [[2]](#ref:molnar))*:\n",
    "\n",
    "1. **Contrastive** - Model explanations must be able to answer *why a given prediction has been made in place of another prediction*. For example, in a model that recommends whether an individual be given a loan or not, a good explanation must be able to tell *what factors should/could I change to alter the model prediction*.\n",
    "2. **Selective** - We *humans are only capable to comprehend 2 to 3 variables at a time*. Thus, a good model explanation must be able to *list the important drivers to explain an outcome*. Imagine having an interpretable model such as linear regression or decision trees, but an explanation that looks at hundreds or thousands of variables, it would be really hard for any human to digest that explanation!\n",
    "3. **Consistent with prior beliefs** - Model explanations are greatly affected by how people perceived them. As such, when *an explanation is consistent with the prior beliefs of an individual*, they tend to favor such explanation ( also known as **confirmation bias**). This is not to discredit any novel or serendipitous discoveries of the model explanation. However, having an explanation that is in line with a domain expert, helps in the model's adoption. Furthermore, if a model is found to exhibit behavior inconsistent with the domain expert's belief, we can enforce constraints on the model or use a linear model that has the required property.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "\n",
    "Among the interpretability methods that you currently know, can you create model explanations that satisfies all of the three characteristics we discussed above?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1295d",
   "metadata": {},
   "source": [
    "## Types of Explainability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8a02b",
   "metadata": {},
   "source": [
    "Model interpretability methods can be classified according to three different criteria (see Figure 5) [[7]](#ref:bbox-peek):\n",
    "\n",
    "<a name='fig:taxonomy'></a>\n",
    "<div>\n",
    "<img src=\"images/taxonomy.png\" align=\"left\" width=\"550\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 5. Taxonomy of different explainability methods.</b><br>\n",
    "           Model interpretability or explainability methods can be intrinsic or post-hoc, model-specific or model-agnostic, local or global. Model-agnostic methods can be further classified whether they use surrogate models or are just visualizations of the behavior of the black-box model.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "First, we can classify whether the method stems from the model being **intrinsically** interpretable. If the model is not intrinsically interpretable, then the explanation method can be applied **post-hoc** or post-model training. Examples of intrinsically interpretable models include: Linear models, Decision Tree, and Rule-based models ([RuleFit](https://github.com/rohan-gt/rulefit) [[8]](#ref:rulefit)).\n",
    "\n",
    "We can further classify the method whether it is **model-specific** or **model-agnostic** (intrinsic explainability methods are model-specific by definition). Model agnostic means that the explainability method can be applied to any black box models. They can be further divided into whether they apply **surrogate** models (e.g., LIME or SHAP Kernel) or is a **visualization** of the behavior of the black box model (e.g., Partial Dependence Plots, Individual Conditional Expectations, or Accumulated Local Effects) [[9]](#ref:med-image).\n",
    "\n",
    "Finally, we can classify an explainability method whether it is a **global** explanation - i.e., it explains the whole model behavior (ex. feature importance and summary visualizations), or whether it is a **local** explanation, i.e., it explains a particular instance in the test or train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56705e0b",
   "metadata": {},
   "source": [
    "## Predictive vs Explanatory Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c60f9d",
   "metadata": {},
   "source": [
    "Two of the models we introduced in machine learning 1 can actually be considered as an intrinsically explainable method, i.e., Decision Trees and Linear Regression models. However, we've only presented them in the context of maximizing the prediction accuracy that we get from these models. For us to reframe their usage for explainability, we need to first differentiate what we mean by using models for prediction versus explanation [[12]](#ref:predict-explain).\n",
    "\n",
    "- **Predictive Modeling** - *process of applying a statistical model or machine learning model to data for the purpose of predicting new or future observations.* As such, the goal for predictive modeling is to minimize the combination of bias and estimation variance to obtain the required empirical precision.\n",
    "\n",
    "- **Explanatory Modeling** - *process of applying statistical models to data for testing causal hypothesis about the theoretical constructs of the data generation process*. The focus for explanatory modeling is to minimize bias to obtain the most accurate representation of the underlying theory.\n",
    "\n",
    "To see how these differ, let's look at two different pipelines in the succeeding subsections - one will focus on predictive modeling (as usual), while the other would heavily focus on explanatory modeling.\n",
    "\n",
    "We'll set our data generation process to be linear for our analysis to emphasize the difference between the two pipeline (see Equation \\ref{eq:data-gen}).\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\beta_2 X_2 ^ 3 + \\beta_1 X_1 + \\beta_0 \\tag{1} \\label{eq:data-gen}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899cccd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:55:49.825408Z",
     "start_time": "2023-01-25T16:55:49.270302Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sklearn make regressions allows us to generate a random regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate 2000 samples using the model:\n",
    "# y = b_2 X_2^3 + b_1 X_1 + 0.77\n",
    "# while adding 4.50 noise to the target.\n",
    "#\n",
    "# Notice we name the output data matrix as X_processed, this is because\n",
    "# we will take the cube root of second feature to simulate a linear\n",
    "# dependence on X_2^3.\n",
    "X_processed, y, coef = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    bias=0.77,\n",
    "    coef=True,\n",
    "    noise=4.50,\n",
    "    random_state=1337\n",
    ")\n",
    "X = np.array([X_processed[:, 0], np.cbrt(X_processed[:, 1])]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f579f",
   "metadata": {},
   "source": [
    "### Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92317050",
   "metadata": {},
   "source": [
    "A predictive modeling pipeline would proceed as follows (for the model experimentation and evaluation step):\n",
    "\n",
    "1. Hold-out test set segregation - setting out a hold-out test set for evaluation\n",
    "2. Model selection - shortlisting which models to use\n",
    "3. Cross validation design - how to perform cross validation\n",
    "4. Hyperparameter tuning - deciding which optimal model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f511b",
   "metadata": {},
   "source": [
    "Let's use 20% of the datapoints as our hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c998edb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:55:49.846044Z",
     "start_time": "2023-01-25T16:55:49.828410Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_holdout, y_trainval, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c123f",
   "metadata": {},
   "source": [
    "Suppose we decided to use the following models:\n",
    "\n",
    "1. Ridge Regression\n",
    "2. Linear SVM\n",
    "3. Nonlinear SVM\n",
    "\n",
    "Then hypertune the `C` or `alpha` of the model along the range of `[1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55924172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:55:49.916824Z",
     "start_time": "2023-01-25T16:55:49.848157Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define C and alpha hyperparameter range\n",
    "C_range = np.logspace(-5, 5, 11)\n",
    "\n",
    "# Prepare the pipeline and parameter grid\n",
    "pipe = Pipeline([('clf', Ridge())])\n",
    "param_grid = [\n",
    "    {'clf': [Ridge(random_state=1337)], 'clf__alpha': C_range},\n",
    "    {'clf': [SVR()], 'clf__kernel': ['linear', 'rbf'], 'clf__C': C_range}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fb101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:37:25.188088Z",
     "start_time": "2023-01-25T16:37:25.180102Z"
    }
   },
   "source": [
    "Next, let's find the optimal model for this case using a 5-fold cross validation strategy using `r2` as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d475ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:56:38.901203Z",
     "start_time": "2023-01-25T16:56:36.280526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is: {'clf': SVR(C=10000.0), 'clf__C': 10000.0, 'clf__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "grid_search.fit(X_trainval, y_trainval)\n",
    "print(f\"The best model is: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a821e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:58:31.563001Z",
     "start_time": "2023-01-25T16:58:31.517223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00350323, 0.00332069, 0.00247359, 0.00553446, 0.00343471,\n",
       "        0.00225654, 0.00148997, 0.00236902, 0.0020648 , 0.0022121 ,\n",
       "        0.0022768 , 0.00412893, 0.00658078, 0.00347638, 0.00460396,\n",
       "        0.00375361, 0.0042872 , 0.00344114, 0.00436811, 0.00286674,\n",
       "        0.00417042, 0.00502763, 0.00344095, 0.00413661, 0.00340056,\n",
       "        0.00638208, 0.00672388, 0.01899447, 0.02276602, 0.05556083,\n",
       "        0.17709327, 0.35269918, 1.4214982 ]),\n",
       " 'std_fit_time': array([1.31848631e-03, 1.27527360e-03, 3.69150624e-04, 2.49912965e-03,\n",
       "        7.51152547e-04, 1.02465018e-03, 4.11271361e-04, 4.13743790e-04,\n",
       "        3.42379094e-04, 3.22542728e-04, 3.30392019e-04, 1.70472318e-04,\n",
       "        3.82092174e-03, 7.82138245e-04, 1.91292209e-03, 6.67074263e-04,\n",
       "        1.28902365e-03, 7.72597610e-04, 1.14362327e-03, 5.46393279e-04,\n",
       "        1.12195306e-03, 2.82226097e-03, 6.60330780e-04, 6.30890005e-04,\n",
       "        6.90575140e-04, 2.74195053e-03, 1.87059907e-03, 6.24161237e-03,\n",
       "        4.20756393e-03, 1.23560137e-02, 3.42191416e-02, 1.97392214e-01,\n",
       "        5.35555982e-01]),\n",
       " 'mean_score_time': array([0.00135827, 0.00124574, 0.00137076, 0.00145068, 0.00108929,\n",
       "        0.0006927 , 0.00076351, 0.00118127, 0.00114832, 0.00133457,\n",
       "        0.00149598, 0.00135589, 0.00178337, 0.00119305, 0.00165372,\n",
       "        0.00129852, 0.00139184, 0.00104289, 0.0014801 , 0.00088167,\n",
       "        0.00144649, 0.00226779, 0.00169082, 0.0011888 , 0.00134006,\n",
       "        0.00160241, 0.00205097, 0.00165515, 0.00213399, 0.00118632,\n",
       "        0.0017117 , 0.00130625, 0.00254307]),\n",
       " 'std_score_time': array([3.51435035e-04, 3.42748347e-04, 2.52964768e-04, 2.44841863e-04,\n",
       "        3.51941467e-04, 1.69198636e-04, 2.19674309e-04, 2.09305529e-04,\n",
       "        2.25344211e-04, 2.12022553e-04, 2.60922513e-04, 2.04120504e-04,\n",
       "        4.93945598e-04, 1.77052083e-04, 7.51993446e-04, 2.49271503e-04,\n",
       "        4.98074328e-04, 1.93175343e-04, 3.34899238e-04, 5.11288629e-05,\n",
       "        6.12605690e-04, 2.72682271e-03, 8.56980019e-04, 5.53298382e-05,\n",
       "        1.92455612e-04, 5.26842852e-04, 9.41553754e-04, 1.18676153e-03,\n",
       "        8.82744717e-05, 5.32268234e-04, 1.36726184e-03, 9.49149992e-04,\n",
       "        1.40712283e-03]),\n",
       " 'param_clf': masked_array(data=[Ridge(random_state=1337), Ridge(random_state=1337),\n",
       "                    Ridge(random_state=1337), Ridge(random_state=1337),\n",
       "                    Ridge(random_state=1337), Ridge(random_state=1337),\n",
       "                    Ridge(random_state=1337), Ridge(random_state=1337),\n",
       "                    Ridge(random_state=1337), Ridge(random_state=1337),\n",
       "                    Ridge(random_state=1337), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0),\n",
       "                    SVR(C=10000.0), SVR(C=10000.0), SVR(C=10000.0)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__alpha': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0,\n",
       "                    1000.0, 10000.0, 100000.0, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__C': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, 1e-05,\n",
       "                    1e-05, 0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1,\n",
       "                    0.1, 1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0,\n",
       "                    1000.0, 10000.0, 10000.0, 100000.0, 100000.0],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__kernel': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, 'linear',\n",
       "                    'rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear',\n",
       "                    'rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear',\n",
       "                    'rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear',\n",
       "                    'rbf', 'linear', 'rbf'],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf': Ridge(random_state=1337), 'clf__alpha': 1e-05},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 0.0001},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 0.001},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 0.01},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 0.1},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 1.0},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 10.0},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 100.0},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 1000.0},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 10000.0},\n",
       "  {'clf': Ridge(random_state=1337), 'clf__alpha': 100000.0},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 1e-05, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 1e-05, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.0001, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.0001, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.001, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.001, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.01, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.01, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.1, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 0.1, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 1.0, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 1.0, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 10.0, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 10.0, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 100.0, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 100.0, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 1000.0, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 1000.0, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 10000.0, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 10000.0, 'clf__kernel': 'rbf'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 100000.0, 'clf__kernel': 'linear'},\n",
       "  {'clf': SVR(C=10000.0), 'clf__C': 100000.0, 'clf__kernel': 'rbf'}],\n",
       " 'split0_test_score': array([ 0.83259048,  0.83259029,  0.83258833,  0.83256877,  0.83237266,\n",
       "         0.83036652,  0.80682431,  0.55400752,  0.10877433,  0.00103455,\n",
       "        -0.01116778, -0.00697818, -0.00698991, -0.00683868, -0.00695596,\n",
       "        -0.00544428, -0.00661645,  0.00843768, -0.00322602,  0.13578503,\n",
       "         0.02812526,  0.64759374,  0.28701894,  0.76981436,  0.6498056 ,\n",
       "         0.78746293,  0.88539253,  0.78801461,  0.9686392 ,  0.78799837,\n",
       "         0.97664823,  0.78784425,  0.93642156]),\n",
       " 'split1_test_score': array([ 0.91595497,  0.91595505,  0.91595585,  0.91596375,  0.91604171,\n",
       "         0.91671857,  0.91499224,  0.69744331,  0.15481397,  0.01144081,\n",
       "        -0.0050107 , -0.00623558, -0.00625114, -0.00605413, -0.00620969,\n",
       "        -0.00424078, -0.00579529,  0.01376903, -0.00166191,  0.17472014,\n",
       "         0.03625673,  0.812579  ,  0.30918209,  0.90753696,  0.7104914 ,\n",
       "         0.91092905,  0.95333376,  0.91174789,  0.97848763,  0.91174829,\n",
       "         0.98682738,  0.91175547,  0.92059947]),\n",
       " 'split2_test_score': array([ 8.69206047e-01,  8.69205926e-01,  8.69204713e-01,  8.69192580e-01,\n",
       "         8.69070645e-01,  8.67792495e-01,  8.50306547e-01,  6.08324379e-01,\n",
       "         1.30921056e-01,  1.07995104e-02, -2.89194596e-03, -3.43744849e-03,\n",
       "        -3.45070358e-03, -3.28202601e-03, -3.41456830e-03, -1.72873310e-03,\n",
       "        -3.05329189e-03,  1.36911797e-02,  5.51836347e-04,  1.54743692e-01,\n",
       "         3.39119566e-02,  7.52879900e-01,  3.04995538e-01,  8.58176101e-01,\n",
       "         6.93557878e-01,  8.62251005e-01,  9.33786693e-01,  8.62249268e-01,\n",
       "         9.64150832e-01,  8.62537978e-01,  9.96041381e-01,  8.62431187e-01,\n",
       "         9.90666844e-01]),\n",
       " 'split3_test_score': array([ 0.82810237,  0.82810287,  0.8281079 ,  0.8281581 ,  0.82865817,\n",
       "         0.83346541,  0.86519215,  0.70078623,  0.01286815, -0.18329908,\n",
       "        -0.20604064, -0.31445994, -0.31448452, -0.3141429 , -0.31438876,\n",
       "        -0.31097525, -0.31343137, -0.27956376, -0.30388633, -0.04037491,\n",
       "        -0.22278046,  0.82539478,  0.27348339,  0.86526773,  0.82941334,\n",
       "         0.85820345,  0.97838022,  0.85820389,  0.98943665,  0.8582042 ,\n",
       "         0.9912947 ,  0.85821185,  0.99108714]),\n",
       " 'split4_test_score': array([ 0.87776857,  0.87776868,  0.87776978,  0.87778073,  0.87788908,\n",
       "         0.87885762,  0.87906874,  0.65906332,  0.11316337, -0.02896264,\n",
       "        -0.04521977, -0.03919601, -0.03921136, -0.03901283, -0.03916636,\n",
       "        -0.03718222, -0.03871646, -0.01899153, -0.0342263 ,  0.13573564,\n",
       "         0.00974792,  0.76451607,  0.31417536,  0.86329238,  0.73295431,\n",
       "         0.86340411,  0.95448087,  0.86356082,  0.99388925,  0.8635553 ,\n",
       "         0.99181362,  0.86349973,  0.96924007]),\n",
       " 'mean_test_score': array([ 0.86472449,  0.86472457,  0.86472531,  0.86473279,  0.86480646,\n",
       "         0.86544012,  0.8632768 ,  0.64392495,  0.10410817, -0.03779737,\n",
       "        -0.05406617, -0.07406143, -0.07407753, -0.07386611, -0.07402707,\n",
       "        -0.07191425, -0.07352257, -0.05253148, -0.06848974,  0.11212192,\n",
       "        -0.02294772,  0.7605927 ,  0.29777106,  0.8528175 ,  0.7232445 ,\n",
       "         0.85645011,  0.94107482,  0.85675529,  0.97892071,  0.85680883,\n",
       "         0.98852506,  0.8567485 ,  0.96160302]),\n",
       " 'std_test_score': array([0.03221357, 0.03221352, 0.03221308, 0.03220864, 0.03216532,\n",
       "        0.03183577, 0.03544757, 0.05599846, 0.04841182, 0.07421964,\n",
       "        0.0775113 , 0.1209093 , 0.1209136 , 0.12084779, 0.12089075,\n",
       "        0.12023347, 0.12066236, 0.11416394, 0.11838715, 0.07759895,\n",
       "        0.10034816, 0.06284191, 0.01521187, 0.04511582, 0.05967129,\n",
       "        0.03952789, 0.03122605, 0.03956783, 0.0114777 , 0.03958158,\n",
       "        0.00661698, 0.03963225, 0.0285928 ]),\n",
       " 'rank_test_score': array([10,  9,  8,  7,  6,  5, 11, 19, 22, 24, 26, 32, 33, 30, 31, 28, 29,\n",
       "        25, 27, 21, 23, 17, 20, 16, 18, 15,  4, 13,  2, 12,  1, 14,  3],\n",
       "       dtype=int32),\n",
       " 'split0_train_score': array([ 8.89677724e-01,  8.89677724e-01,  8.89677724e-01,  8.89677713e-01,\n",
       "         8.89676699e-01,  8.89577255e-01,  8.81311673e-01,  6.51437673e-01,\n",
       "         1.45517023e-01,  1.63977327e-02,  1.66054599e-03, -1.22073304e-03,\n",
       "        -1.23455905e-03, -1.05603886e-03, -1.19429098e-03,  5.90043303e-04,\n",
       "        -7.91680523e-04,  1.69649018e-02,  3.22739223e-03,  1.69170450e-01,\n",
       "         4.20726371e-02,  7.66077127e-01,  3.37368124e-01,  8.70086759e-01,\n",
       "         7.44298942e-01,  8.78995022e-01,  9.64717890e-01,  8.79215161e-01,\n",
       "         9.95898109e-01,  8.79209362e-01,  9.97712680e-01,  8.79156592e-01,\n",
       "         9.98043764e-01]),\n",
       " 'split1_train_score': array([ 8.68263853e-01,  8.68263853e-01,  8.68263853e-01,  8.68263843e-01,\n",
       "         8.68262918e-01,  8.68172155e-01,  8.60581184e-01,  6.42978685e-01,\n",
       "         1.45877729e-01,  1.64897534e-02,  1.67043243e-03,  5.81621815e-06,\n",
       "        -7.36846017e-06,  1.63585790e-04,  3.17483061e-05,  1.74027372e-03,\n",
       "         4.22828847e-04,  1.74063743e-02,  4.32492208e-03,  1.62932836e-01,\n",
       "         4.20978440e-02,  7.44100065e-01,  3.35473848e-01,  8.42682927e-01,\n",
       "         7.31228446e-01,  8.48729245e-01,  9.56077653e-01,  8.49701692e-01,\n",
       "         9.96570063e-01,  8.49702283e-01,  9.97909557e-01,  8.49712237e-01,\n",
       "         9.98189388e-01]),\n",
       " 'split2_train_score': array([ 8.77485368e-01,  8.77485368e-01,  8.77485367e-01,  8.77485358e-01,\n",
       "         8.77484406e-01,  8.77390993e-01,  8.69590490e-01,  6.47724710e-01,\n",
       "         1.46221920e-01,  1.65102683e-02,  1.67230276e-03, -5.39150623e-05,\n",
       "        -6.79228920e-05,  1.13685859e-04, -2.63829442e-05,  1.78866557e-03,\n",
       "         3.88926835e-04,  1.84402311e-02,  4.53305478e-03,  1.70037657e-01,\n",
       "         4.42189789e-02,  7.68914941e-01,  3.44290860e-01,  8.54939935e-01,\n",
       "         7.41864587e-01,  8.56904770e-01,  9.59653802e-01,  8.56902594e-01,\n",
       "         9.95006271e-01,  8.57197817e-01,  9.97794934e-01,  8.57237369e-01,\n",
       "         9.98082019e-01]),\n",
       " 'split3_train_score': array([ 0.87868327,  0.87868327,  0.87868327,  0.87868326,  0.87868236,\n",
       "         0.87859401,  0.87118001,  0.65506297,  0.15017078,  0.0170141 ,\n",
       "         0.00172399, -0.00540922, -0.0054231 , -0.00524542, -0.00538417,\n",
       "        -0.00360846, -0.00499503,  0.01265837, -0.00111327,  0.1598211 ,\n",
       "         0.03495802,  0.75876117,  0.30490182,  0.85976359,  0.7451833 ,\n",
       "         0.86332358,  0.95294718,  0.86332328,  0.99635265,  0.86332565,\n",
       "         0.99843577,  0.86334202,  0.99864213]),\n",
       " 'split4_train_score': array([ 8.75069676e-01,  8.75069676e-01,  8.75069676e-01,  8.75069666e-01,\n",
       "         8.75068679e-01,  8.74971798e-01,  8.66909607e-01,  6.41527888e-01,\n",
       "         1.43369628e-01,  1.61533868e-02,  1.63576512e-03, -3.19816396e-04,\n",
       "        -3.32614591e-04, -1.67331997e-04, -2.95306002e-04,  1.35665348e-03,\n",
       "         7.77082345e-05,  1.65106575e-02,  3.80068529e-03,  1.60078738e-01,\n",
       "         4.02225218e-02,  7.43815326e-01,  3.31020935e-01,  8.53841452e-01,\n",
       "         7.20341820e-01,  8.57414890e-01,  9.49327896e-01,  8.57868312e-01,\n",
       "         9.95415770e-01,  8.57857513e-01,  9.97933774e-01,  8.57749227e-01,\n",
       "         9.98093481e-01]),\n",
       " 'mean_train_score': array([ 8.77835979e-01,  8.77835979e-01,  8.77835979e-01,  8.77835969e-01,\n",
       "         8.77835013e-01,  8.77741242e-01,  8.69914592e-01,  6.47746385e-01,\n",
       "         1.46231415e-01,  1.65130478e-02,  1.67260742e-03, -1.39957380e-03,\n",
       "        -1.41311236e-03, -1.23830411e-03, -1.37368080e-03,  3.73436062e-04,\n",
       "        -9.79448445e-04,  1.63961060e-02,  2.95455636e-03,  1.64408157e-01,\n",
       "         4.07140011e-02,  7.56333726e-01,  3.30611117e-01,  8.56262932e-01,\n",
       "         7.36583419e-01,  8.61073501e-01,  9.56544884e-01,  8.61402207e-01,\n",
       "         9.95848572e-01,  8.61458525e-01,  9.97957343e-01,  8.61439488e-01,\n",
       "         9.98210156e-01]),\n",
       " 'std_train_score': array([6.93319241e-03, 6.93319241e-03, 6.93319240e-03, 6.93319216e-03,\n",
       "        6.93316761e-03, 6.93077701e-03, 6.74895874e-03, 5.07068335e-03,\n",
       "        2.20653273e-03, 2.80804239e-04, 2.88024488e-05, 2.05236679e-03,\n",
       "        2.05255852e-03, 2.05087116e-03, 2.05278144e-03, 2.03667461e-03,\n",
       "        2.05506835e-03, 1.97516811e-03, 2.08361059e-03, 4.38935549e-03,\n",
       "        3.14383868e-03, 1.06347176e-02, 1.35467688e-02, 8.89250461e-03,\n",
       "        9.52725822e-03, 1.00934451e-02, 5.32372250e-03, 9.90584483e-03,\n",
       "        5.78598700e-04, 9.87834489e-03, 2.52177197e-04, 9.86224752e-03,\n",
       "        2.21246733e-04])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65981efe",
   "metadata": {},
   "source": [
    "More often than not, when we focus on predictive accuracy, we sacrifice theoretical accuracy for improved empirical precision. Indeed, when it comes to choosing the model with the highest predictive accuracy, the optimal model are the least interpretable. (see Figure <a href='#fig:accuracy-interpretability'>2</a>) [[1]](#ref:interpret-ml).\n",
    "\n",
    "<a name='fig:accuracy-interpretability'></a>\n",
    "<div>\n",
    "<img src=\"images/accuracy-interpretability-trade-off.PNG\" align=\"left\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 2. Machine learning model accuracy and interpretability tradeoff.</b><br>\n",
    "           Models that are highly accurate are the least interpretable, while models that are highly interpretable have a sub-par accuracy [<a href='#ref:interpret-ml'>1</a>].\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088ddb7",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccff2fa",
   "metadata": {},
   "source": [
    "<a name='ref:interpret-ml'></a> [1] Guo, Mengzhuo, et al. \"An interpretable machine learning framework for modelling human decision behavior.\" *arXiv preprint arXiv:1906.01233* (2019).\n",
    "\n",
    "<a name='ref:molnar'></a> [2] Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/.\n",
    "\n",
    "<a name='ref:miller'></a> [3] Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” *arXiv Preprint arXiv:1706.07269.* (2017)\n",
    "\n",
    "<a name='ref:kim'></a> [4] Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” *Advances in Neural Information Processing Systems* (2016).\n",
    "\n",
    "<a name='ref:crisp-dm'></a> [5] Kelleher, John D., Brian Mac Namee, and Aoife D’Arcy. \"Machine Learning for Predictive Analytics: The Predictive Data Analytics Project Lifecycle: CRISP-DM.\" *Fundamentals of machine learning for predictive analytics*, The MIT Press, 2020, pp. 15-17.\n",
    "\n",
    "<a name='ref:great-ai-debate'></a> [6] NeurIPS 2017. “The Great AI Debate - NIPS2017 - Yann LeCun.” *YouTube*, uploaded by The Artificial Intelligence Channel, 1 February 2018, https://youtu.be/93Xv8vJ2acI.\n",
    "\n",
    "<a name='ref:bbox-peek'></a> [7] Adadi, Amina, and Mohammed Berrada. \"Peeking inside the black-box: a survey on explainable artificial intelligence (XAI).\" *IEEE access 6* (2018): 52138-52160.\n",
    "\n",
    "<a name='ref:rulefit'></a> [8] Friedman, Jerome H., and Bogdan E. Popescu. \"Predictive learning via rule ensembles.\" The Annals of Applied Statistics 2.3 (2008): 916-954.\n",
    "\n",
    "<a name='ref:med-image'></a> [9] Singh, Amitojdeep, Sourya Sengupta, and Vasudevan Lakshminarayanan. \"Explainable deep learning models in medical image analysis.\" *Journal of Imaging 6.6* (2020): 52.\n",
    "\n",
    "<a name='ref:shap-how'></a> [10] Mazzanti, Samuele. \"SHAP Values Explained Exactly How You Wished Someone Explained to You.\" *Towards Data Science*, 04 Apr. 2020, https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30\n",
    "\n",
    "<a name='ref:shapley-handbook'></a> [11] Algaba, Encarnación, Vito Fragnelli, and Joaquín Sánchez-Soriano, eds. *Handbook of the Shapley value*. CRC Press, 2019.\n",
    "\n",
    "<a name='ref:predict-explain'></a> [12] Shmueli G. *To explain or to predict?*. Statistical science. 2010 Aug;25(3):289-310.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
